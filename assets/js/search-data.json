{
  
    
        "post0": {
            "title": "Distributed Computation and the Tao of Parsimony",
            "content": "My job, and my hobby, is thinking about how computation works. How do numbers flow through your computer, go through some arcane numeric witchery, and wind up doing exactly what you need them to do? . It helps to imagine a computer in the same way a famous science fiction novel does; in the book The Three Body Problem, by Cixin Liu, humanity builds a computer by using people holding flags to represent information. . Armies of people grouped together on a field waving flags can pretend (moderately well) to be a computation engine. Pretty neat. . One of the ways that large amounts of information were moved back and forth was by using a soldier on horseback (a buffer), to carry information from one group to another quickly. . You can&#39;t simply allow everyone to run around on horseback! It would be chaos and nothing would get done; everyone would be in the way. . That doesn&#39;t quite literally translate to modern computing, but the metaphor is solid. . You can imagine computation as interconnected groups of people; each group can perform a task. The groups are connected by wires and information can be sent down the wires, but only as fast as a person can talk. Additionally, the information the group holds is written on stone tablets; each person can only hold literally so much information. Also, the group is limited is to how quickly they can solve the problem. . These concepts have corollaries; SerDe (Serialization / Deserialization) refers to the actions taken to put information into, and pull things out of, a buffer which allows processes to exchange information. It takes time to create an item in the buffer and it takes time to read the item in the buffer. . There&#39;s an information transfer speed limit. . Each process is using compute from at least one of potentially many processors but is bound to the processing power available in the process (well, kind of, Spark and other distributed engines skirt this constraint.) . Each process requires holding the data it gets from the buffer, meaning that you could have two processes holding the same information. These processes potentially share the same resources and data exchanged between them will require both processes to each maintain a copy. . Inefficient. . But, good news awaits! Engineers have done their best to work around this problem in a way that lets you do what you need to do by using no-copy. . What is No Copy? . Numpy, and PyArrow, are two data container and computation solutions which provide a high-level interface to low-level highly optimized and efficient computation. . Imagine that the group of people in the example above all maintain their stone tablets in one place and every group has a wire connecting it to that place. When one group is done with a job and they want to share the stone tablets they&#39;ve made they tell the other group where the tablets are. . A reference to the location in memory of an array that PyArrow and / or Numpy are maintaining is what Numpy and PyArrow are handing back and forth, which allows them to transfer the location quickly without copying the data. Another process just slaps a Numpy / PyArrow sticker on that location and says &#39;look at how many bits can fit in that trunk&#39;. . The information speed limit just got cranked way up. . How much gain was there? . Let&#39;s find out; I&#39;m going to run an experiment below where I simulate pulling information out of a Ray cluster. . As long as the Ray storage location is on the same device Numpy gets the advantage of no-copy when Ray hands Numpy arrays around. . Does PyArrow get the same? What about just using a regular buffer? We&#39;ll simulate pulling Numpy, PyArrow, and List containers and see what the time and resource usage statistics look like. . Disclosure; this was run on a commodity laptop. . Experimental Setup . Note that this is all local, meaning that this is a one-node cluster. . ## Random number generation import numpy.random as nr ## Pyarrow! import pyarrow as pa ## Experiment tracking import mlflow ## Remote computation import ray ## Just used to help organize temporary output. import tempfile ## Measurement from guppy import hpy ## Measurment import cProfile ## Measurement import io ## Measurement import pstats ## Visualization import seaborn as sns # Experiment Instantiation ## Tell MLFlow to use a local SQLite DB. Will be created if DNE. mlflow.set_tracking_uri(&#39;sqlite:////experiments/experiments.db&#39;) ## Create a new experiment mlflow.set_experiment(&#39;pyarrow-ray-01&#39;) ## Tell Numpy how to deal out pseudo-randomness. rng = nr.default_rng(10) ## Turn on the Ray cluster. ray.init() . 2022-04-09 11:24:14,946 WARNING services.py:1909 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing &#39;--shm-size=0.90gb&#39; to &#39;docker run&#39; (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM. . {&#39;node_ip_address&#39;: &#39;172.17.0.2&#39;, &#39;raylet_ip_address&#39;: &#39;172.17.0.2&#39;, &#39;redis_address&#39;: None, &#39;object_store_address&#39;: &#39;/tmp/ray/session_2022-04-09_11-24-13_821050_1765/sockets/plasma_store&#39;, &#39;raylet_socket_name&#39;: &#39;/tmp/ray/session_2022-04-09_11-24-13_821050_1765/sockets/raylet&#39;, &#39;webui_url&#39;: None, &#39;session_dir&#39;: &#39;/tmp/ray/session_2022-04-09_11-24-13_821050_1765&#39;, &#39;metrics_export_port&#39;: 50628, &#39;gcs_address&#39;: &#39;172.17.0.2:59219&#39;, &#39;address&#39;: &#39;172.17.0.2:59219&#39;, &#39;node_id&#39;: &#39;a230bbbacfd40433cad2a3a11f80560b6ffbba64b803dc7004ed8483&#39;} . Experiment . This experiment simulates extracting significant amounts of data from the Ray cluster. . Various array sizes ($S$) are run through ten trials ($T in(0, 9)$); in each trial an array of size $S$ is pulled $1000T$ times from the cluster. The RAM usage and the time for this trial are logged. . @ray.remote def random_data(n): &quot;&quot;&quot;Produce random data for testing.&quot;&quot;&quot; return list(rng.random(n)) @ray.remote def random_data_numpy(n): &quot;&quot;&quot;Produce random data for testing.&quot;&quot;&quot; return rng.random(n) @ray.remote def random_data_pyarrow(n): &quot;&quot;&quot;Produce rand(om data for testing.&quot;&quot;&quot; return pa.array(rng.random(n)) def trial(remote_data, m): &quot;&quot;&quot;Estimate the length of time it takes to get a set of items out of a Ray cluster.&quot;&quot;&quot; i = 0 while i &lt;= m: ray.get(remote_data) . Run the experiment . mlflow.set_experiment(&#39;pyarrow-ray-01&#39;) # Start measurement of RAM usage h = hpy() # Declare the heap x = h.heap() # Loop through the sizes with tempfile.TemporaryDirectory() as t: for array_size in [100, 1000, 10000, 100000]: print(f&quot;Running experiment with array_size = {array_size}&quot;) # Start the experimental run for trial in range(10): with mlflow.start_run() as experiment_run: mlflow.log_param(key=&#39;array_size&#39;, value=array_size) print(f&quot;Running trial = {trial}&quot;) mlflow.log_param(key=&#39;trial&#39;, value=trial) remote_data = { &#39;list&#39;: random_data.remote(array_size), &#39;numpy&#39;: random_data_numpy.remote(array_size), &#39;pyarrow&#39;: random_data_pyarrow.remote(array_size) } for key, value in remote_data.items(): print(f&quot;Collecting data for = {key}&quot;) def get_val(): for _ in range(trial * 1000): _ = ray.get(value) # Pre function call RAM usage: size_0 = x.size # Profile the function call pr = cProfile.Profile() pr.enable() get_val() pr.disable() # Post function RAM usage. s = io.StringIO() ps = pstats.Stats(pr, stream=s) mlflow.log_metric(key=f&#39;time_{key}&#39;, value=ps.print_stats().total_tt) mlflow.log_metric(key=f&#39;ram_{key}&#39;, value=ps.print_stats().total_tt) . Running experiment with array_size = 100 Running trial = 0 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 1 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 2 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 3 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 4 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 5 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 6 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 7 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 8 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 9 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running experiment with array_size = 1000 Running trial = 0 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 1 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 2 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 3 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 4 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 5 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 6 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 7 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 8 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 9 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running experiment with array_size = 10000 Running trial = 0 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 1 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 2 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 3 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 4 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 5 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 6 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 7 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 8 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 9 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running experiment with array_size = 100000 Running trial = 0 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 1 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 2 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 3 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 4 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 5 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 6 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 7 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 8 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow Running trial = 9 Collecting data for = list Collecting data for = numpy Collecting data for = pyarrow . Unpack the results . Here we&#39;re going to unpack the information! . from mlflow import search_runs df_results = search_runs() . df_results = df_results[[&#39;metrics.time_pyarrow&#39;, &#39;metrics.time_numpy&#39;, &#39;metrics.time_list&#39;, &#39;metrics.ram_pyarrow&#39;, &#39;metrics.ram_numpy&#39;, &#39;metrics.ram_list&#39;, &#39;params.array_size&#39;, &#39;params.trial&#39;]] . df_results.head() . metrics.time_pyarrow metrics.time_numpy metrics.time_list metrics.ram_pyarrow metrics.ram_numpy metrics.ram_list params.array_size params.trial . 0 3.127627 | 3.000348 | 611.496308 | 3.127627 | 3.000348 | 611.496308 | 100000 | 9 | . 1 2.595407 | 2.448557 | 604.117492 | 2.595407 | 2.448557 | 604.117492 | 100000 | 8 | . 2 2.916762 | 2.924818 | 561.058394 | 2.916762 | 2.924818 | 561.058394 | 100000 | 7 | . 3 1.874686 | 1.515688 | 400.770934 | 1.874686 | 1.515688 | 400.770934 | 100000 | 6 | . 4 1.321852 | 1.304269 | 333.754422 | 1.321852 | 1.304269 | 333.754422 | 100000 | 5 | . Computation Time Profiling . This section lays out two plots; the first just helps understand how long it takes to get regular items in and out of the buffer in this experiment by just showing List item times. The second picture contrasts the time by representing the PyArrow and Numpy time as a fraction of the time that a List took. . What do these pictures illustrate? . Putting lists into and pulling them out from a remote storage by copying them into and out of a buffer is dumb. Don&#39;t do it. Note the log scale on the illustration and how the lines appear to be uniformly separated? The array sizes increase as powers of 10 and so we can use the picture to illustrate that this the retrieval time is linearly proportional to the size of the array (there are better ways of showing this, but this plot helps uniformly separate the lines.) Basically $t_{transfer}=aS$ where $a$ is some undefined constant and $S$ is the size of the array. | Numpy and PyArrow both get about the same amount of performance gain w.r.t. copying data into and out of the buffer. | Boom! . Now when we build distributed computation engines we can use Numpy and PyArrow data structures. . import matplotlib.pyplot as plt sns.set_style(&#39;white&#39;, {&quot;axes.facecolor&quot;: &quot;.9&quot;}) sns.set_context(&#39;notebook&#39;) ax = sns.lineplot( x=&#39;params.trial&#39;, y=&quot;metrics.time_list&quot;, hue=&#39;params.array_size&#39;, data=df_results.astype({&#39;params.array_size&#39;:&#39;int&#39;, &#39;params.trial&#39;:&#39;int&#39;}).query(&#39;`params.trial`&gt;0&#39;), palette=&quot;Set2&quot; ) ax.set_xlabel(&quot;Number of Queries (x1000)&quot;) ax.set_ylabel(&quot;Time (s)&quot;) ax.set_yscale(&quot;log&quot;) ax.set_title(&quot;How long does it take to buffer items?&quot;) plt.legend(title=&#39;Array_Size&#39;, loc=&#39;center&#39;) sns.set(rc = {&#39;figure.figsize&#39;:(8,3)}) . As a fair word of warning this plot could be mildly deceptive; the only information that you should take from it is that the distribution of times for both PyArrow and Numpy item retrieval are kind of hand-wavy approximately equal. . ax = sns.histplot( x=&#39;Time&#39;, hue=&#39;Container Type&#39;, data=df_results.eval( &#39;PyArrow=`metrics.time_pyarrow`/`metrics.time_list`&#39; ).eval( &#39;Numpy=`metrics.time_numpy`/`metrics.time_list`&#39; ).melt( id_vars=[&#39;params.array_size&#39;, &#39;params.trial&#39;], value_vars=[&#39;PyArrow&#39;, &#39;Numpy&#39;], var_name=&#39;Container Type&#39;, value_name=&#39;Time&#39; ).astype({&#39;params.array_size&#39;:&#39;int&#39;, &#39;params.trial&#39;:&#39;int&#39;}).query(&#39;`params.trial`&gt;0 and `params.array_size`&gt;100&#39;), palette=&quot;Set2&quot;, multiple=&#39;dodge&#39;, bins=20 ) ax.set_xlabel(&quot;Time (s)&quot;) ax.set_ylabel(&quot;Count&quot;) ax.set_title(&quot;&quot;&quot;How relatively efficient are Numpy and PyArrow? Both PyArrow and Numpy are portrayed *relative* to the time for a list query.&quot;&quot;&quot;) sns.set(rc = {&#39;figure.figsize&#39;:(8,3)}) .",
            "url": "https://lunarengineer.github.io/LunarEngineerBlog/distributed%20computation/pyarrow/numpy/2022/04/09/PyArrowRay_01.html",
            "relUrl": "/distributed%20computation/pyarrow/numpy/2022/04/09/PyArrowRay_01.html",
            "date": " • Apr 9, 2022"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "Algorithms, Data Structures, And What the Hell Those Have to do with Learning",
            "content": "Introduction . &quot;I&#39;d like to do something amazing.&quot; . That&#39;s not a very descriptive thing to say, but sometimes it&#39;s good enough to get you started. . What takes your dream from this half-assed idea of a mission statement to something tangible? How do you do that? . I guess one way you could do that would be to make a list of exact rules that you use every time to appropriately act in any situation. Then, you could follow these rules exactly. . Well, that&#39;s an algorithm. . math = &quot;dumb&quot; timeline = { &#39;age_of_mandated_retirement_withdrawal&#39;: 72, &#39;mandated withdrawal amount&#39;, &#39;tax bracket at that time&#39;, &#39;when sell lvnworth house (its income)&#39;, &#39;does selling house change income bracket&#39; } . Algorithm . An algorithm, according to Dasgupta and crew, is: . Precise, | Unambiguous | A few other things. | It&#39;s a step-by-step, paint-by-numbers, solution for the problem it&#39;s intended to solve and it normally uses math to lay out the solution. . But in order to be able to do you need to be able to talk about things in a really concrete way. You need to be able to speak about the world in precise and unambiguous ways. . In order to do that you create data structures. . Data Structure . Sets deep dive A data structure is a rigid framework for expressing how bits of information are stored and accessed. It&#39;s a way to represent an Item, or even a Set containing many Items. . I&#39;m going to make up a data structure now and I&#39;ll call it a Managed Information Forest. . Data structures - rigid framework for expressing how bits of information work with each other (e.g. list, or linked list) Learning - . Chat about algorithms and data structures and why they&#39;re generally important. Speak towards efficiency and mention tractibility. . Talk about data requirements of deep learners and how stochastic gradient descent requires tons of data. . Data Structures . A data structure is equatable to a se of formal rules for how I choose to define some thing. . Let&#39;s say that I want to close my eyes and imagine that I&#39;ve got a new friend Bob, who is the number 6. . Atomic Data Structures . bob = 6 . Because I say that Bob is the number 6 I completely define everything about Bob. I know what I&#39;m allowed to do with Bob. . Btw, Bob is an Integer. . print(f&quot;Bob is an {type(bob)}&quot;) # You could run this command to get a fascinating look into how Bob works. # help(bob) # You can think of dir as &#39;What can you do&#39;? print(f&quot;I have {len(dir(bob))} properties and methods&quot;) . Bob is an &lt;class &#39;int&#39;&gt; I have 71 properties and methods . I can add Bob to Bob or to Susan, who is also an integer. . susan = 14 bob + susan . 20 . bob + 100*bob . 606 . A data structure houses information (the value six) and all the tools to access or use the information (defines how to add, subtract, etc.) . An integer is a data structure, as well as being a data type. . Rabbit Hole : https://en.wikipedia.org/wiki/Integer_(computer_science) . Container Data Structures . Comes the time when we want to talk about a bunch of numbers together, generally collecting similar numbers together. . Basic container structures let us start examining things taken in relation to one another. . I can line everyone up by size, or shape, or cut of their jib. Whatever. . Container strucutres range from simple tuples and lists and dictionaries to more complex structures like B Trees. . Hash Table . Data Structures . Hash Table | Binary Tree | Red Black Trees | . Binary Tree . Hash Tables Binary Trees Red Black Trees . https://www.usenix.org/system/files/conference/atc16/atc16_paper-mitchell.pdf . https://cs.uwaterloo.ca/~kmsalem/courses/CS848W10/presentations/Eflov-BTree.pdf . https://edutechlearners.com/download/Introduction_to_algorithms-3rd%20Edition.pdf . https://www.cs.dartmouth.edu/~thc/ . https://medium.com/javarevisited/10-best-books-for-data-structure-and-algorithms-for-beginners-in-java-c-c-and-python-5e3d9b478eb1 .",
            "url": "https://lunarengineer.github.io/LunarEngineerBlog/2022/01/09/AlgAndDataStructures.html",
            "relUrl": "/2022/01/09/AlgAndDataStructures.html",
            "date": " • Jan 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Banjo Pig",
            "content": ". Lego Creations . My son is creative and loves Legos; this particular banjo playing pig he created would love to lay down a beat for you! .",
            "url": "https://lunarengineer.github.io/LunarEngineerBlog/ben/funny/lego/creator/2022/01/07/Banjo-Pig.html",
            "relUrl": "/ben/funny/lego/creator/2022/01/07/Banjo-Pig.html",
            "date": " • Jan 7, 2022"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "TL;DR: Individual attention is critical for learning. Everyone should have it. . There, now you can skip the rest. . An idiots tale, told in five parts. . The rest of this is laid out to help understand my motivation and why it is personal. I’ll meander from my motivations into a personal story and tie it all together in a neat bow at the end. . What motivates me . My family . My family is my everything and I would burn the world down for them if I needed to. Thankfully, I can avoid prison because they need something different right now. They need a good dad. I try. They generally forgive me. . Being able to look into the future doesn’t make you a good dad, but being able to prepare your children for that future (so they have the best chance of success) makes you a slightly better dad than otherwise. . Their future . My kids are not being brought into a life of comfort. They’re being faced with a future where food shortages are quite likely, where climate refugees are stressing already crumbling social infrastructure and where extreme weather events bring crippling infrastructure damages. . That’s already happening. . . Within the next fifty years the best case scenarios don’t look great. Civilizations in areas where it’s already difficult to sustain life are likely to crumble entirely as their environment becomes hostile to life, meaning that there will be at least some small countries which functionally cease to exist. . The humanitarian burden from those climate refugees will cause a rippling cascade of system failure as neighboring countries also fail if they choose to assume the burden. . These are problems that we need to solve, quickly. Many people are working on them now, trying to solve them diligently. They’re going to need some help. . Now, we’re going to take a short dog leg where I explain why this is personal but, don’t worry, I bring it back home in the end. . I’ve been young and dumb . I have a very strong memory of earning a plastic travel mug when I was in first grade. There aren’t many memories of the school there (touching a lamb’s brain, the layout of the school, watching a pottery demonstration) but I have a few solid memories. This is one of them. . I wasn’t doing well. I’m not entirely sure why not, or even in which subject (math?), but I remember that a new teacher came in and sat with me, personally, for a good amount of time. She ‘incentived’ (bribed) me with that mug, provided that I work hard. . I earned the mug and I’m assuming that my grades improved because she did not come back. . I continued to struggle into high school (somewhere around the time that I chose to stop medicating myself for ADD). My grades generally were not good. I remember a fourth grade report card; F, F, D, and F. I think that teacher got fired, though. I vividly remember failing Food Skills and Algebra 2 in high school. . I graduated high school with a 2.1 GPA. Only two others graduated in my class with lower grades than me. . After I graduated high school I spent two years not really doing anything. I drifted from job to job, I wound up withdrawing from online schooling that I wasn’t doing, and in the end I decided to join the Army on a whim. . The next eight years were spent in the Army learning a lot about who I was, who I wanted to be, and about how other people work. I also learned many ways not to do things. . I was convinced that I wasn’t necessarily very bright but starting to get an inkling that maybe I wasn’t a complete idiot; I wanted to try something new. I also, quite possibly, wanted to compete with my sister, the doctor. So, I thought to myself ‘what is the hardest thing you could do?’, and that’s why I went back to school for aerospace engineering. . Four years of being a full time student while trying to be a husband and father marked one of the hardest times in my life. I struggled and, in the end, graduated. . Was I really dumb? . The short answer is: ‘probably not’. . I got a 30 on my ACT when I took it in High School. I did not study. I read Jurassic Park when I was in fourth grade simply because I thought the cover was neat. I spent most of my class time illicitly reading. I got a 99 on my ASVAB when I joined the Army, for what that’s worth (not much.) I’ve learned, and mostly forgotten, more than one language. My grades in technical school in the military were remarkably good. There are other indicators like the fact that the same semester I failed food skills I did ok in physics (I said ok, not great!), chem 2, and business law. . My undergraduate was very challenging for personal reasons but learning the material generally wasn’t difficult. My grades did start to slip when I was maintaining a job on top of a full load of upper level engineering courses, while my wife was in school, too. . That wasn’t fun, though we overcame that struggle in the end. . I think I treasure three things that I learned in my undergraduate work above all: . I can learn new things pretty easily, and I love learning. | A friend taught me how to carry a coffee cup while walking without spilling (seriously blew my mind). | I don’t give a flying F. about airplanes. Not one. I do like math, though. | . . Now I’ve got an M.S. in computer science on top of a B.S. in aerospace engineering (plus a nifty associates degree I nabbed from my language school) and I work as a Data Scientist. I spend every day thinking for a living and I’m at least ok at my job. . If I’d had someone who could work with me personally more often would things have worked out differently? Would I have fallen in love with learning sooner if I had someone who could answer any question I asked? . What would be different if I’d had individual attention? . What if I’d been invested because I was interested? What if someone spent time with me, engaging me? . Would it have made a difference? . It doesn’t really matter for me, because it’s in the past. . But, let me tell you a story. . Little Bobby sits down at the kitchen table to do his homework. . He doesn’t care about much beyond playing, but he’s going to go on someday to win a Nobel for developing a drought resistant variant of grain. . Now, let me tell you a couple billion stories about a few billion little Bobbys. Imagine them as a literal constellation of two billion stars. (Approximately two billion people on earth are under age 14.) . Economic impoverishment is almost insurmountable for children. There goes about ten percent, so I want you to imagine ten percent of those lights winking out. A lot of other people still have opportunity, but those one in ten stars (200 million) just went dark. More of them just don’t have access to education period so knock that down, again, by around 3% of the original number (60 million). Take some untold number of them who learning just doesn’t click for and more stars dim. There are a lot of interesting statistics relevant to education in the world and it looks like they boil down to: . Access to high quality education is not guaranteed. | Access to individualized attention in education is not guaranteed and is expensive. | . Watch some more of those lights flicker and fade. . A good chunk of people do not continue on to secondary education. | Individual attention is not feasible for a human to provide at all times. | . More lights. . In the end, there are too many lights dark. . There are some other interesting statistics: . Education is correlated with satisfaction, income and success. | Education is correlated with a code of morality that views the self as not always being the most important thing. | . How can we provide everyone with the same standard for learning? How can we keep all the lights on? How can we give everyone individual attention? . Everyone deserves individual attention . Answers to questions are generally just a Google query away. We’re raising the future to be comfortable with the idea of having information readily available and dropping them into a sea of knowledge. We should at least help them by gaming the system in their favor and providing every child with a personalized recommendation system that can reliably provide each child the most appropriate learning materials for them to achieve their educational goal. . But everyone learns differently. . Human learning theory revolves around this. Some learning theories are appropriate for certain students, but not for others. For some children having individualized attention is required for them to succeed. . My children and their future are dearly important to me and I’m getting older every day. I’ve lived a life full of stupid at nearly every turn and I’ve somehow managed to come out on top, though far later than I believe I should have. . I was medicated for ADD until I decided (at about age 15) that I was done taking the medication. It’s around that time that my grades (already poor) took a nose dive. I failed Algebra 2 in sophomore year of high school. . I ask myself, fairly often, what if I’d had someone that could teach me and understand me? Would I have failed? Where would I be? . Everyone deserves to be provided the potential for greatness. Everyone deserves individual, readily accessible, education. Everyone deserves a chance to make a difference. Everyone deserves a fair shot in life. . So that’s where we are, now. . Let’s fix all the problems. . I’m working to solve the problems of the future, today. I probably sound egotistical, or crazy, but that doesn’t really matter and I don’t care anymore. I’ve got nothing really left to prove, beyond another piece of paper. What’s the point of a PhD? Research a topic sufficiently to provide new learning in an area and appropriately source your material such that it has credibility. Great, that’s what I’m planning on doing. Would it be nice to have a piece of paper acknowledging what I did? Sure. Wouldn’t it be better to just do it, though? . I develop statistical learning applications for a living and have the skillset to go from formulating a hypothesis to design of experiments to understanding how those results support / disprove my hypothesis. I can develop containerized applications and I have relevant experience developing scalable computation. I have an understanding of machine learning and computer science. . There’s nothing in this pipeline that I can’t do alone. I could do it faster with friends. . My research is primarily focused on couching human learning theory in a statistical learning framework and optimizing the rate of learning in humans. . I aim to provide every child with a ‘Buddy AI’ whose sole purpose is to support that child with information that is most likely to resonate with them and be helpful. Some kids learn by doing, some by reading, some by watching. This algorithm will learn to approximate the child’s learning style and will provide material which is predicted to maximize how quickly that child uses the information to build skills. . Let’s go light up the stars. .",
          "url": "https://lunarengineer.github.io/LunarEngineerBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Executive Summary",
          "content": "Individualized educational assistance at scale is not feasible, especially for economically impoverished areas. . The goal of Lunar Engineering is to aid in transforming the educational industry by providing autonomous individualized educational assistance at scale. . This is achieved using an algorithm to build and maintain a tree data structure used to sample information from in order to maximize the rate of learner information gain and retention affected by intelligent sampling. This is used in a recommendation system. . In less technical terms, think of this like a Librarian, of sorts, that learns to anticipate which training material is most likely to help a learner master a topic. . Success in this project is marked by all learners being optimized for growth, measured via an educational framework. .",
          "url": "https://lunarengineer.github.io/LunarEngineerBlog/exsum/",
          "relUrl": "/exsum/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lunarengineer.github.io/LunarEngineerBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}